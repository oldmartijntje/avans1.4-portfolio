{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate analysis: Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://media.geeksforgeeks.org/wp-content/uploads/20240919170522/supervised-vs-reinforcement-vs-unsupervised.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://docs.microsoft.com/en-us/azure/machine-learning/team-data-science-process/media/overview/tdsp-lifecycle2.png \"Data Science Lifecycle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting sources: \n",
    "- https://learn.datacamp.com/courses/machine-learning-with-tree-based-models-in-python\n",
    "- https://www.datacamp.com/community/tutorials/decision-tree-classification-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen that we can uncover correlations by using bivariate analyis. This also raises the question: If we extract information from multiple columns (Multivariate analysis), could we use these correlations to calculate/predict the value of a column for rows that do not yet have a value?\n",
    "\n",
    "For example:\n",
    "- Can we calculate if a customer will churn (= We lose the customer)?\n",
    "- Can we calculate if a customer would use a certain product? (Product recommendation)\n",
    "- Can we calculate if a mail is spam or not?\n",
    "- Can we calculate if a financial transaction is fraudulent or not?\n",
    "- Can we calculate if a customer will be able to pay back their loan or not?\n",
    "- Can we calculate the price people are willing to pay for a house?\n",
    "- Can we calculate the salary a student will earn in the future?\n",
    "- ...\n",
    "\n",
    "Just as we used math/statistics to support to explore the data one column at a time (Univariate) and per combination of two columns (Bivariate analysis), we will use machine learning algorithms to extract information across multiple columns (Multivariate analysis) and to build data products. \n",
    "\n",
    "Machine learning is:\n",
    "- Machine learning is a method of data analysis that automates analytical model building. It is a branch of artificial intelligence based on the idea that systems can learn from data, identify patterns and make decisions with minimal human intervention. (https://www.sas.com/en_us/insights/analytics/machine-learning.html#:~:text=Machine%20learning%20is%20a%20method,decisions%20with%20minimal%20human%20intervention.)\n",
    "- Machine learning (ML) is the study of computer algorithms that improve automatically through experience.[1] It is seen as a part of artificial intelligence. Machine learning algorithms build a model based on sample data, known as \"training data\", in order to make predictions or decisions without being explicitly programmed to do so.[2] Machine learning algorithms are used in a wide variety of applications, such as email filtering and computer vision, where it is difficult or unfeasible to develop conventional algorithms to perform the needed tasks. (https://en.wikipedia.org/wiki/Machine_learning)\n",
    "\n",
    "Examples of data products could include:\n",
    "- A product recommender\n",
    "- A customer churn predictor\n",
    "- A mail labeler (Primary, promotion, social, spam, etc.)\n",
    "- Sentiment analyser for social media messages\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/max/1204/1*qYoU2VTBsORAfhkfiluURQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will refer to the column that we are trying to calculate as the <b>target variable</b>. The columns used for this calculation will be refered to as the <b>feature variables</b>.\n",
    "\n",
    "For example, if we want to predict a person's shoe size based on their body length:\n",
    "- The shoe size is the target variable\n",
    "- The body length is the feature variable\n",
    "\n",
    "Extra info: The terms 'target' and 'feature' are borrowed from the field of Machine Learning. In the field of statistics, we refer to the target variable as the dependent variable and we refer to the feature variables as the independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the target variable is a categorical variable then we refer to this task as a classification task.\n",
    "\n",
    "Examples of classification tasks:\n",
    "- Predict if a customer will churn. \n",
    "- Predict if a customer will use a specific product.\n",
    "- Label mails as spam.\n",
    "- Label financial transactions as fraudulent.\n",
    "- Label a social media message as positive, neutral or negative.\n",
    "- Predict if a customer of the bank will be able to pay back the loan.\n",
    "\n",
    "Examples of machine learning algorithms that we could use for classification:\n",
    "- Decision trees\n",
    "- Random forests\n",
    "- Logistic regression\n",
    "- Neural networks\n",
    "- Naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "iris = sns.load_dataset(\"iris\")\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/max/1000/1*Hh53mOF4Xy4eORjLilKOwA.png \"Iris dataset\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features= ['sepal_length', 'petal_length']\n",
    "dt = DecisionTreeClassifier(max_depth = 2) # Increase max_depth to see effect in the plot\n",
    "dt.fit(iris[features], iris['species'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Python code trains a **Decision Tree Classifier** on a subset of the famous Iris dataset, using only two features: 'sepal_length' and 'petal_length'. **Only these will be used to train the model**. It creates a decision tree classifier with a **maximum depth of 5**, meaning the tree will be allowed to split at most 5 times from the root to a leaf. This limits its complexity and helps prevent overfitting.\n",
    "\n",
    "It trains (fits) the decision tree on the selected features and the **target label 'species'**. It learns to classify the type of Iris flower (setosa, versicolor, virginica) based on 'sepal_length' and 'petal_length'.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`conda install -c anaconda graphviz`  \n",
    "`conda install -c anaconda python-graphviz`  \n",
    "`conda install -c anaconda pydot`  \n",
    "If the code below still does not work, install from here: (https://graphviz.org/download/)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "import graphviz\n",
    "\n",
    "def plot_tree_classification(model, features, class_names):\n",
    "    # Generate plot data\n",
    "    dot_data = tree.export_graphviz(model, out_file=None, \n",
    "                          feature_names=features,  \n",
    "                          class_names=class_names,  \n",
    "                          filled=True, rounded=True,  \n",
    "                          special_characters=True)  \n",
    "\n",
    "    # Turn into graph using graphviz\n",
    "    graph = graphviz.Source(dot_data)  \n",
    "\n",
    "    # Write out a pdf\n",
    "    graph.render(\"decision_tree\")\n",
    "\n",
    "    # Display in the notebook\n",
    "    return graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "plot_tree_classification(dt, features, np.sort(iris.species.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What do you see here?**\n",
    "\n",
    "Let's take a look at the first node:\n",
    "\n",
    "samples = 150: all samples from the dataset.\n",
    "value = [50, 50, 50]: 50 samples each from setosa, versicolor, and virginica.\n",
    "gini = 0.667: high impurity (which makes sense, since the classes are perfectly balanced).\n",
    "petal_length â‰¤ 2.45: this is the first split criterion.\n",
    "\n",
    "class = the predicted class.\n",
    "The class with the highest number of samples at that node.\n",
    "But at the first node, all classes are equally represented ([50, 50, 50]), so technically the model is just picking one arbitrarily â€” probably the first in lexicographic order, which is: setosa, versicolor, virginica\n",
    "So, it's not a prediction, just a default label when there's a tie.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = dt.predict(iris[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What this does:**\n",
    "\n",
    "It uses your trained decision tree model (dt) to make predictions on the dataset iris[features], which contains only the two features:\n",
    "'sepal_length' and 'petal_length'.\n",
    "\n",
    "**ðŸ§  What kind of predictions?**\n",
    "\n",
    "The model outputs the predicted species (e.g., 'setosa', 'versicolor', 'virginica') for each row in the dataset â€” based on the decision rules it learned during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(predictions, actuals):\n",
    "    if(len(predictions) != len(actuals)):\n",
    "        raise Exception(\"The amount of predictions did not equal the amount of actuals\")\n",
    "    \n",
    "    return (predictions == actuals).sum() / len(actuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_accuracy(predictions, iris.species)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function **calculate_accuracy(predictions, actuals)** computes the classification accuracy of your model's predictions compared to the actual (true) labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test/train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_train, iris_test = train_test_split(iris, test_size=0.3, random_state=42, stratify=iris['species'])\n",
    "print(iris_train.shape, iris_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why Test/Train split?**\n",
    "\n",
    "Splitting the dataset allows you to train the model on one part (training set) and evaluate it on another (test set).\n",
    "This helps you simulate how the model performs on unseen, real-world data. It prevents **overfitting**, where the model memorizes the training data but fails on new data.\n",
    "\n",
    "The **stratify** option ensures that each class is proportionally represented in both the training and test sets. This is important for balanced model training and fair evaluation.\n",
    "**random_state ensures reproducibility**, so you get the same split each time you run the code.\n",
    "**A 70/30 split (test_size=0.3)** is a common and balanced choice for small datasets like Iris."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features= ['sepal_length', 'sepal_width']\n",
    "dt_classification = DecisionTreeClassifier(max_depth = 3) # Increase max_depth to see effect in the plot\n",
    "dt_classification.fit(iris_train[features], iris_train['species'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionsOnTrainset = dt_classification.predict(iris_train[features])\n",
    "predictionsOnTestset = dt_classification.predict(iris_test[features])\n",
    "\n",
    "accuracyTrain = calculate_accuracy(predictionsOnTrainset, iris_train.species)\n",
    "accuracyTest = calculate_accuracy(predictionsOnTestset, iris_test.species)\n",
    "\n",
    "print(\"Accuracy on training set \" + str(accuracyTrain))\n",
    "print(\"Accuracy on test set \" + str(accuracyTest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ“Š**Results Summary**\n",
    "\n",
    "Training Accuracy: 83.8%\n",
    "Test Accuracy: 73.3%\n",
    "\n",
    "ðŸ§ **What this means**\n",
    "\n",
    "Your model performs reasonably well on the training data.\n",
    "The lower test accuracy suggests the model is not generalizing perfectly to unseen data.\n",
    "The ~10% drop in accuracy is a sign of mild overfitting â€” the model may have learned some patterns specific to the training data that don't hold in the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Portfolio assignment 15\n",
    "30 min: Train a decision tree to predict the species of a **penguin** based on their characteristics.\n",
    "- Prepare the data:\n",
    "    - <b>Note</b>: Some machine learning algorithms can not handle missing values. You will either need to: \n",
    "         - replace missing values (with the mean or most popular value). For replacing missing values you can use .fillna(\\<value\\>) https://pandas.pydata.org/docs/reference/api/pandas.Series.fillna.html\n",
    "         - remove rows with missing data.  You can remove rows with missing data with .dropna() https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html\n",
    "- Split the penguin dataset into a train (70%) and test (30%) set.\n",
    "- Use the train set to fit a DecisionTreeClassifier. You are free to to choose which columns you want to use as feature variables and you are also free to choose the max_depth of the tree. \n",
    "- Use your decision tree model to make predictions for both the train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins = sns.load_dataset(\"penguins\")\n",
    "penguins.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/0v1CGNV.png)<br>\n",
    "- Calculate the accuracy for both the train set predictions and test set predictions\n",
    "- Is the accurracy different? Did you expect this difference?\n",
    "- Use the plot_tree_classification function above to create a plot of the decision tree. Take a few minutes to analyse the decision tree. Do you understand the tree?\n",
    "- Which depth and features did you add per cycle?\n",
    "- Optional: Perform the same tasks but try to predict the sex of the pinguin based on the other columns<br>\n",
    "<br>\n",
    "\n",
    "Findings: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Portfolio assignment 16\n",
    "30 min: Train a decision tree to predict one of the categorical columns of your **own** dataset.\n",
    "- Prepare the data:<br>\n",
    "    - <b>Note</b>: Some machine learning algorithms can not handle missing values. You will either need to: \n",
    "         - replace missing values (with the mean or most popular value). For replacing missing values you can use .fillna(\\<value\\>) https://pandas.pydata.org/docs/reference/api/pandas.Series.fillna.html\n",
    "         - remove rows with missing data.  You can remove rows with missing data with .dropna() https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html <br>\n",
    "- Split your dataset into a train (70%) and test (30%) set.\n",
    "- Use the train set to fit a DecisionTreeClassifier. You are free to to choose which columns you want to use as feature variables and you are also free to choose the max_depth of the tree. \n",
    "- Use your decision tree model to make predictions for both the train and test set.<br>\n",
    "<br>\n",
    "    \n",
    "![](https://i.imgur.com/0v1CGNV.png)<br>\n",
    "- Calculate the accuracy for both the train set predictions and test set predictions.\n",
    "- Is the accurracy different? Did you expect this difference?\n",
    "- Use the plot_tree function above to create a plot of the decision tree. Take a few minutes to analyse the decision tree. Do you understand the tree?\n",
    "- Which depth and features did you add per cycle?<br>\n",
    "<br>\n",
    "\n",
    "Findings: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the target variable is a numerical variable then we refer to this task as a regression task.\n",
    "\n",
    "Examples of regression tasks:\n",
    "- Predict the price people are willing to pay for a house.\n",
    "- Predict the salary a student will earn in the future.\n",
    "\n",
    "Examples of machine learning algorithms that we could use for regression:\n",
    "- Linear regression\n",
    "- Decision trees\n",
    "- Random forests\n",
    "- Neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.corr(numeric_only=True) # to find out which features  correlate with sepal_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features= ['petal_length'] # add 'petal_width' ('species' does not work; categorical is not implemented in DT of sciki learn)\n",
    "dt_regression = DecisionTreeRegressor(max_depth = 2) # Increase max_depth to see effect in the plot\n",
    "dt_regression.fit(iris_train[features].values, iris_train['sepal_length'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualisation in a scatterplot how a decision tree regressor makes his decisions\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X_train=iris_train[features].values                     # Assign matrix X\n",
    "y_train=iris_train['sepal_length'].values               # Assign matrix y\n",
    "\n",
    "sort_idx = X_train.flatten().argsort()                  # Sort X and y by ascending values of X\n",
    "X_train = X_train[sort_idx]\n",
    "y_train = y_train[sort_idx]\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.scatter(X_train, y_train, c='steelblue',                  # Plot actual target against features\n",
    "            edgecolor='white', s=70)\n",
    "plt.plot(X_train, dt_regression.predict(X_train),                      # Plot predicted target against features\n",
    "         color='red', lw=2)\n",
    "plt.xlabel('petal_length')\n",
    "plt.ylabel('sepal_length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install dtreeviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import dtreeviz\n",
    "viz_rmodel = dtreeviz.model(dt_regression, X_train, y_train,\n",
    "                            feature_names=features,\n",
    "                            target_name='sepal_length')\n",
    "viz_rmodel.rtree_feature_space(features=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "import graphviz\n",
    "\n",
    "def plot_tree_regression(model, features):\n",
    "    # Generate plot data\n",
    "    dot_data = tree.export_graphviz(model, out_file=None, \n",
    "                          feature_names=features,  \n",
    "                          filled=True, rounded=True,  \n",
    "                          special_characters=True)  \n",
    "\n",
    "    # Turn into graph using graphviz\n",
    "    graph = graphviz.Source(dot_data)  \n",
    "\n",
    "    # Write out a pdf\n",
    "    graph.render(\"decision_tree\")\n",
    "\n",
    "    # Display in the notebook\n",
    "    return graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tree_regression(dt_regression, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://docs.oracle.com/en/cloud/saas/freeform/ffuuu/img/insights_rmse_formula.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rmse(predictions, actuals):\n",
    "    if(len(predictions) != len(actuals)):\n",
    "        raise Exception(\"The amount of predictions did not equal the amount of actuals\")\n",
    "    \n",
    "    return (((predictions - actuals) ** 2).sum() / len(actuals)) ** (1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionsOnTrainset = dt_regression.predict(iris_train[features])\n",
    "predictionsOnTestset = dt_regression.predict(iris_test[features])\n",
    "\n",
    "rmseTrain = calculate_rmse(predictionsOnTrainset, iris_train.sepal_length)\n",
    "rmseTest = calculate_rmse(predictionsOnTestset, iris_test.sepal_length)\n",
    "\n",
    "print(\"RMSE on training set \" + str(rmseTrain))\n",
    "print(\"RMSE on test set \" + str(rmseTest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "âœ… How to interpret this:\n",
    "Root Mean Squared Error (RMSE) gives the average prediction error in the same units as your target variable (here: sepal_length, measured in cm).\n",
    "\n",
    "An RMSE of ~0.46 cm means your model's average error on the test set is less than half a centimeter â€” quite acceptable, given that sepal_length values range roughly between 4.3 and 7.9 cm in the Iris dataset.\n",
    "\n",
    "The small gap between train and test RMSE also indicates low overfitting and good generalization.\n",
    "\n",
    "ðŸ“Œ Rule of thumb:\n",
    "To judge whether RMSE is \"good\", compare it to:\n",
    "\n",
    "The range or standard deviation of the target variable.\n",
    "A baseline model, like predicting the mean sepal length for every sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Portfolio assignment 17\n",
    "30 min: Train a decision tree to predict the body_mass_g of a penguin based on their characteristics.\n",
    "- Split the penguin dataset into a train (70%) and test (30%) set.\n",
    "- Use the train set to fit a DecisionTreeRegressor. You are free to to choose which columns you want to use as feature variables and you are also free to choose the max_depth of the tree. \n",
    "<b>Note</b>: Some machine learning algorithms can not handle missing values. You will either need to \n",
    "     - replace missing values (with the mean or most popular value). For replacing missing values you can use .fillna(\\<value\\>) https://pandas.pydata.org/docs/reference/api/pandas.Series.fillna.html\n",
    "     - remove rows with missing data.  You can remove rows with missing data with .dropna() https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html\n",
    "- Use your decision tree model to make predictions for both the train and test set.<br>\n",
    "<br>\n",
    "\n",
    "![](https://i.imgur.com/0v1CGNV.png)<br>\n",
    "- Which depth and features did you add per cycle?\n",
    "- Calculate the RMSE for both the train set predictions and test set predictions.\n",
    "- Is the RMSE different? Did you expect this difference?\n",
    "- Use the plot_tree_regression function above to create a plot of the decision tree. Take a few minutes to analyse the decision tree. Do you understand the tree?<br>\n",
    "<br>\n",
    "\n",
    "Findings: ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins = sns.load_dataset(\"penguins\")\n",
    "penguins.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Portfolio assignment 18\n",
    "30 min: Train a decision tree to predict one of the numerical columns of your own dataset.\n",
    "- Split your dataset into a train (70%) and test (30%) set.\n",
    "- Use the train set to fit a DecisionTreeRegressor. You are free to to choose which columns you want to use as feature variables and you are also free to choose the max_depth of the tree. \n",
    "- Use your decision tree model to make predictions for both the train and test set.<br>\n",
    "<br>\n",
    "\n",
    "![](https://i.imgur.com/0v1CGNV.png)<br>\n",
    "- Which depth and features did you add per cycle?\n",
    "- Calculate the RMSE for both the train set predictions and test set predictions.\n",
    "- Is the RMSE different? Did you expect this difference?\n",
    "- Use the plot_tree_regression function above to create a plot of the decision tree. Take a few minutes to analyse the decision tree. Do you understand the tree?<br>\n",
    "<br>\n",
    "\n",
    "Findings: ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
