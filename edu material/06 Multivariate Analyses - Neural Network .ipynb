{
 "cells": [
  {
   "cell_type": "code",
   "id": "7b8c5890-558c-4f33-90fb-eeaf0e99cdf0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T11:52:44.420741Z",
     "start_time": "2025-05-13T11:52:44.364900Z"
    }
   },
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "# Ignore the ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "ee88e7a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T11:52:45.864115Z",
     "start_time": "2025-05-13T11:52:44.509577Z"
    }
   },
   "source": [
    "pip install -r requirements.txt"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jupyter in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from -r requirements.txt (line 1)) (1.1.1)\n",
      "Requirement already satisfied: notebook in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from -r requirements.txt (line 2)) (7.4.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from -r requirements.txt (line 3)) (2.2.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from -r requirements.txt (line 4)) (2.2.5)\n",
      "Requirement already satisfied: scipy in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from -r requirements.txt (line 5)) (1.15.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from -r requirements.txt (line 6)) (3.10.1)\n",
      "Requirement already satisfied: seaborn in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from -r requirements.txt (line 7)) (0.13.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from -r requirements.txt (line 8)) (1.6.1)\n",
      "Requirement already satisfied: graphviz in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from -r requirements.txt (line 9)) (0.20.3)\n",
      "Requirement already satisfied: dtreeviz in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from -r requirements.txt (line 10)) (2.2.2)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from -r requirements.txt (line 11)) (4.11.0.86)\n",
      "Requirement already satisfied: ultralytics in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from -r requirements.txt (line 12)) (8.3.123)\n",
      "Requirement already satisfied: jupyter-console in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from jupyter->-r requirements.txt (line 1)) (6.6.3)\n",
      "Requirement already satisfied: nbconvert in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from jupyter->-r requirements.txt (line 1)) (7.16.6)\n",
      "Requirement already satisfied: ipykernel in c:\\users\\oldma\\appdata\\roaming\\python\\python313\\site-packages (from jupyter->-r requirements.txt (line 1)) (6.29.5)\n",
      "Requirement already satisfied: ipywidgets in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from jupyter->-r requirements.txt (line 1)) (8.1.6)\n",
      "Requirement already satisfied: jupyterlab in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from jupyter->-r requirements.txt (line 1)) (4.4.1)\n",
      "Requirement already satisfied: jupyter-server<3,>=2.4.0 in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from notebook->-r requirements.txt (line 2)) (2.15.0)\n",
      "Requirement already satisfied: jupyterlab-server<3,>=2.27.1 in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from notebook->-r requirements.txt (line 2)) (2.27.3)\n",
      "Requirement already satisfied: notebook-shim<0.3,>=0.2 in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from notebook->-r requirements.txt (line 2)) (0.2.4)\n",
      "Requirement already satisfied: tornado>=6.2.0 in c:\\users\\oldma\\appdata\\roaming\\python\\python313\\site-packages (from notebook->-r requirements.txt (line 2)) (6.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\oldma\\appdata\\roaming\\python\\python313\\site-packages (from pandas->-r requirements.txt (line 3)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from pandas->-r requirements.txt (line 3)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from pandas->-r requirements.txt (line 3)) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from matplotlib->-r requirements.txt (line 6)) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from matplotlib->-r requirements.txt (line 6)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from matplotlib->-r requirements.txt (line 6)) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from matplotlib->-r requirements.txt (line 6)) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\oldma\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib->-r requirements.txt (line 6)) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from matplotlib->-r requirements.txt (line 6)) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from matplotlib->-r requirements.txt (line 6)) (3.2.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from scikit-learn->-r requirements.txt (line 8)) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from scikit-learn->-r requirements.txt (line 8)) (3.6.0)\n",
      "Requirement already satisfied: colour in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from dtreeviz->-r requirements.txt (line 10)) (0.1.5)\n",
      "Requirement already satisfied: pytest in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from dtreeviz->-r requirements.txt (line 10)) (8.3.5)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from ultralytics->-r requirements.txt (line 12)) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.23.0 in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from ultralytics->-r requirements.txt (line 12)) (2.32.3)\n",
      "Requirement already satisfied: torch>=1.8.0 in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from ultralytics->-r requirements.txt (line 12)) (2.7.0)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from ultralytics->-r requirements.txt (line 12)) (0.22.0)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from ultralytics->-r requirements.txt (line 12)) (4.67.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\oldma\\appdata\\roaming\\python\\python313\\site-packages (from ultralytics->-r requirements.txt (line 12)) (7.0.0)\n",
      "Requirement already satisfied: py-cpuinfo in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from ultralytics->-r requirements.txt (line 12)) (9.0.0)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.0 in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from ultralytics->-r requirements.txt (line 12)) (2.0.14)\n",
      "Requirement already satisfied: anyio>=3.1.0 in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 2)) (4.9.0)\n",
      "Requirement already satisfied: argon2-cffi>=21.1 in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 2)) (23.1.0)\n",
      "Requirement already satisfied: jinja2>=3.0.3 in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 2)) (3.1.6)\n",
      "Requirement already satisfied: jupyter-client>=7.4.4 in c:\\users\\oldma\\appdata\\roaming\\python\\python313\\site-packages (from jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 2)) (8.6.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\oldma\\appdata\\roaming\\python\\python313\\site-packages (from jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 2)) (5.7.2)\n",
      "Requirement already satisfied: jupyter-events>=0.11.0 in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 2)) (0.12.0)\n",
      "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 2)) (0.5.3)\n",
      "Requirement already satisfied: nbformat>=5.3.0 in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 2)) (5.10.4)\n",
      "Requirement already satisfied: overrides>=5.0 in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 2)) (7.7.0)\n",
      "Requirement already satisfied: prometheus-client>=0.9 in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 2)) (0.21.1)\n",
      "Requirement already satisfied: pywinpty>=2.0.1 in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 2)) (2.0.15)\n",
      "Requirement already satisfied: pyzmq>=24 in c:\\users\\oldma\\appdata\\roaming\\python\\python313\\site-packages (from jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 2)) (26.4.0)\n",
      "Requirement already satisfied: send2trash>=1.8.2 in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 2)) (1.8.3)\n",
      "Requirement already satisfied: terminado>=0.8.3 in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 2)) (0.18.1)\n",
      "Requirement already satisfied: traitlets>=5.6.0 in c:\\users\\oldma\\appdata\\roaming\\python\\python313\\site-packages (from jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 2)) (5.14.3)\n",
      "Requirement already satisfied: websocket-client>=1.7 in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 2)) (1.8.0)\n",
      "Requirement already satisfied: async-lru>=1.0.0 in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from jupyterlab->jupyter->-r requirements.txt (line 1)) (2.0.5)\n",
      "Requirement already satisfied: httpx>=0.25.0 in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from jupyterlab->jupyter->-r requirements.txt (line 1)) (0.28.1)\n",
      "Requirement already satisfied: jupyter-lsp>=2.0.0 in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from jupyterlab->jupyter->-r requirements.txt (line 1)) (2.2.5)\n",
      "Requirement already satisfied: setuptools>=41.1.0 in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from jupyterlab->jupyter->-r requirements.txt (line 1)) (75.8.0)\n",
      "Requirement already satisfied: comm>=0.1.1 in c:\\users\\oldma\\appdata\\roaming\\python\\python313\\site-packages (from ipykernel->jupyter->-r requirements.txt (line 1)) (0.2.2)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in c:\\users\\oldma\\appdata\\roaming\\python\\python313\\site-packages (from ipykernel->jupyter->-r requirements.txt (line 1)) (1.8.14)\n",
      "Requirement already satisfied: ipython>=7.23.1 in c:\\users\\oldma\\appdata\\roaming\\python\\python313\\site-packages (from ipykernel->jupyter->-r requirements.txt (line 1)) (9.1.0)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in c:\\users\\oldma\\appdata\\roaming\\python\\python313\\site-packages (from ipykernel->jupyter->-r requirements.txt (line 1)) (0.1.7)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\oldma\\appdata\\roaming\\python\\python313\\site-packages (from ipykernel->jupyter->-r requirements.txt (line 1)) (1.6.0)\n",
      "Requirement already satisfied: babel>=2.10 in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from jupyterlab-server<3,>=2.27.1->notebook->-r requirements.txt (line 2)) (2.17.0)\n",
      "Requirement already satisfied: json5>=0.9.0 in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from jupyterlab-server<3,>=2.27.1->notebook->-r requirements.txt (line 2)) (0.12.0)\n",
      "Requirement already satisfied: jsonschema>=4.18.0 in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from jupyterlab-server<3,>=2.27.1->notebook->-r requirements.txt (line 2)) (4.23.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from nbconvert->jupyter->-r requirements.txt (line 1)) (4.13.4)\n",
      "Requirement already satisfied: bleach!=5.0.0 in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from bleach[css]!=5.0.0->nbconvert->jupyter->-r requirements.txt (line 1)) (6.2.0)\n",
      "Requirement already satisfied: defusedxml in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from nbconvert->jupyter->-r requirements.txt (line 1)) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from nbconvert->jupyter->-r requirements.txt (line 1)) (0.3.0)\n",
      "Requirement already satisfied: markupsafe>=2.0 in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from nbconvert->jupyter->-r requirements.txt (line 1)) (3.0.2)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from nbconvert->jupyter->-r requirements.txt (line 1)) (3.1.3)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from nbconvert->jupyter->-r requirements.txt (line 1)) (0.10.2)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from nbconvert->jupyter->-r requirements.txt (line 1)) (1.5.1)\n",
      "Requirement already satisfied: pygments>=2.4.1 in c:\\users\\oldma\\appdata\\roaming\\python\\python313\\site-packages (from nbconvert->jupyter->-r requirements.txt (line 1)) (2.19.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\oldma\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 3)) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from requests>=2.23.0->ultralytics->-r requirements.txt (line 12)) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from requests>=2.23.0->ultralytics->-r requirements.txt (line 12)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from requests>=2.23.0->ultralytics->-r requirements.txt (line 12)) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from requests>=2.23.0->ultralytics->-r requirements.txt (line 12)) (2025.1.31)\n",
      "Requirement already satisfied: filelock in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from torch>=1.8.0->ultralytics->-r requirements.txt (line 12)) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from torch>=1.8.0->ultralytics->-r requirements.txt (line 12)) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from torch>=1.8.0->ultralytics->-r requirements.txt (line 12)) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from torch>=1.8.0->ultralytics->-r requirements.txt (line 12)) (3.4.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from torch>=1.8.0->ultralytics->-r requirements.txt (line 12)) (2025.3.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\oldma\\appdata\\roaming\\python\\python313\\site-packages (from tqdm>=4.64.0->ultralytics->-r requirements.txt (line 12)) (0.4.6)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from ipywidgets->jupyter->-r requirements.txt (line 1)) (4.0.14)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.14 in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from ipywidgets->jupyter->-r requirements.txt (line 1)) (3.0.14)\n",
      "Requirement already satisfied: prompt-toolkit>=3.0.30 in c:\\users\\oldma\\appdata\\roaming\\python\\python313\\site-packages (from jupyter-console->jupyter->-r requirements.txt (line 1)) (3.0.51)\n",
      "Requirement already satisfied: iniconfig in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from pytest->dtreeviz->-r requirements.txt (line 10)) (2.1.0)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from pytest->dtreeviz->-r requirements.txt (line 10)) (1.5.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 2)) (1.3.1)\n",
      "Requirement already satisfied: argon2-cffi-bindings in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 2)) (21.2.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert->jupyter->-r requirements.txt (line 1)) (0.5.1)\n",
      "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from bleach[css]!=5.0.0->nbconvert->jupyter->-r requirements.txt (line 1)) (1.4.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from httpx>=0.25.0->jupyterlab->jupyter->-r requirements.txt (line 1)) (1.0.8)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from httpcore==1.*->httpx>=0.25.0->jupyterlab->jupyter->-r requirements.txt (line 1)) (0.14.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\oldma\\appdata\\roaming\\python\\python313\\site-packages (from ipython>=7.23.1->ipykernel->jupyter->-r requirements.txt (line 1)) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in c:\\users\\oldma\\appdata\\roaming\\python\\python313\\site-packages (from ipython>=7.23.1->ipykernel->jupyter->-r requirements.txt (line 1)) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\oldma\\appdata\\roaming\\python\\python313\\site-packages (from ipython>=7.23.1->ipykernel->jupyter->-r requirements.txt (line 1)) (0.19.2)\n",
      "Requirement already satisfied: stack_data in c:\\users\\oldma\\appdata\\roaming\\python\\python313\\site-packages (from ipython>=7.23.1->ipykernel->jupyter->-r requirements.txt (line 1)) (0.6.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook->-r requirements.txt (line 2)) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook->-r requirements.txt (line 2)) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook->-r requirements.txt (line 2)) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook->-r requirements.txt (line 2)) (0.24.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\oldma\\appdata\\roaming\\python\\python313\\site-packages (from jupyter-core!=5.0.*,>=4.12->jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 2)) (4.3.7)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\users\\oldma\\appdata\\roaming\\python\\python313\\site-packages (from jupyter-core!=5.0.*,>=4.12->jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 2)) (310)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 2)) (3.3.0)\n",
      "Requirement already satisfied: rfc3339-validator in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 2)) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 2)) (0.1.1)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from nbformat>=5.3.0->jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 2)) (2.21.1)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\oldma\\appdata\\roaming\\python\\python313\\site-packages (from prompt-toolkit>=3.0.30->jupyter-console->jupyter->-r requirements.txt (line 1)) (0.2.13)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics->-r requirements.txt (line 12)) (1.3.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from beautifulsoup4->nbconvert->jupyter->-r requirements.txt (line 1)) (2.7)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\oldma\\appdata\\roaming\\python\\python313\\site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyter->-r requirements.txt (line 1)) (0.8.4)\n",
      "Requirement already satisfied: fqdn in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 2)) (1.5.1)\n",
      "Requirement already satisfied: isoduration in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 2)) (20.11.0)\n",
      "Requirement already satisfied: jsonpointer>1.13 in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 2)) (3.0.0)\n",
      "Requirement already satisfied: uri-template in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 2)) (1.3.0)\n",
      "Requirement already satisfied: webcolors>=24.6.0 in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 2)) (24.11.1)\n",
      "Requirement already satisfied: cffi>=1.0.1 in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 2)) (1.17.1)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\oldma\\appdata\\roaming\\python\\python313\\site-packages (from stack_data->ipython>=7.23.1->ipykernel->jupyter->-r requirements.txt (line 1)) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\oldma\\appdata\\roaming\\python\\python313\\site-packages (from stack_data->ipython>=7.23.1->ipykernel->jupyter->-r requirements.txt (line 1)) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\oldma\\appdata\\roaming\\python\\python313\\site-packages (from stack_data->ipython>=7.23.1->ipykernel->jupyter->-r requirements.txt (line 1)) (0.2.3)\n",
      "Requirement already satisfied: pycparser in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 2)) (2.22)\n",
      "Requirement already satisfied: arrow>=0.15.0 in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 2)) (1.3.0)\n",
      "Requirement already satisfied: types-python-dateutil>=2.8.10 in c:\\users\\oldma\\anaconda3\\envs\\datascience\\lib\\site-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook->-r requirements.txt (line 2)) (2.9.0.20241206)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "0dd4350c-5285-4985-b6cd-d1b0f574f3ed",
   "metadata": {},
   "source": [
    "# Artificial Neural Networks \n",
    "\n",
    "## About this notebook\n",
    "\n",
    "This notebook kernel was created to help you understand more about machine learning. I intend to create tutorials with several machine learning algorithms from basic to advanced. I hope I can help you with this data science trail. For any information, you can contact me through the link below.\n",
    "\n",
    "Contact me here: https://www.linkedin.com/in/vitorgamalemos/\n",
    "\n",
    "## Introduction \n",
    "\n",
    "<img src=\"https://media.springernature.com/original/springer-static/image/art%3A10.1007%2Fs40846-016-0191-3/MediaObjects/40846_2016_191_Fig1_HTML.gif\">\n",
    "\n",
    "<p style=\"text-align: justify;\">Artificial Neural Networks are mathematical models inspired by the human brain, specifically the ability to learn, process, and perform tasks. The Artificial Neural Networks are powerful tools that assist in solving complex problems linked mainly in the area of combinatorial optimization and machine learning. In this context, artificial neural networks have the most varied applications possible, as such models can adapt to the situations presented, ensuring a gradual increase in performance without any human interference. We can say that the Artificial Neural Networks are potent methods can give computers a new possibility, that is, a machine does not get stuck to preprogrammed rules and opens up various options to learn from its own mistakes.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5ea60e-cf16-46fa-9335-197f00b53f73",
   "metadata": {},
   "source": [
    "## Biologic Model\n",
    "\n",
    "<img src=\"https://www.neuroskills.com/images/photo-500x500-neuron.png\">\n",
    "<p style=\"text-align: justify;\">Artificial neurons are designed to mimic aspects of their biological counterparts. The neuron is one of the fundamental units that make up the entire brain structure of the central nervous system; such cells are responsible for transmitting information through the electrical potential difference in their membrane. In this context, a biological neuron can be divided as follows.</p>\n",
    "\n",
    "**Dendrites** – are thin branches located in the nerve cell. These cells act on receiving nerve input from other parts of our body.\n",
    "\n",
    "**Soma** – acts as a summation function. As positive and negative signals (exciting and inhibiting, respectively) arrive in the soma from the dendrites they are added together.\n",
    "\n",
    "**Axon** – gets its signal from the summation behavior which occurs inside the soma. It is formed by a single extended filament located throughout the neuron. The axon is responsible for sending nerve impulses to the external environment of a cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42de27de-036c-446f-940b-c7ab943cbca5",
   "metadata": {},
   "source": [
    "## Artificial Neuron as Mathematic Notation\n",
    "In general terms, an input X is multiplied by a weight W and added a bias b producing the net activation. \n",
    "<img style=\"max-width:60%;max-height:60%;\" src=\"https://miro.medium.com/max/1290/1*-JtN9TWuoZMz7z9QKbT85A.png\">\n",
    "\n",
    "We can summarize an artificial neuron with the following mathematical expression:\n",
    "$$\n",
    "\\hat{y} = f\\left(\\text{net}\\right)= f\\left(\\vec{w}\\cdot\\vec{x}+b\\right) = f\\left(\\sum_{i=1}^{n}{w_i x_i + b}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1073dc3b-0cd9-425c-87ea-aec322435563",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## The Singlelayer Perceptron\n",
    "\n",
    "<p style=\"text-align: justify;\">The Perceptron and its learning algorithm pioneered the research in neurocomputing. the perceptron is an algorithm for supervised learning of binary classifiers [1]. A binary classifier is a function which can decide whether or not an input, represented by a vector of numbers, belongs to some specific class. It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector.<p>\n",
    "    \n",
    "<img src=\"https://www.edureka.co/blog/wp-content/uploads/2017/12/Perceptron-Learning-Algorithm_03.gif\">\n",
    "    \n",
    "#### References\n",
    "    \n",
    "- Freund, Y.; Schapire, R. E. (1999). \"Large margin classification using the perceptron algorithm\" (PDF). Machine Learning\n",
    "\n",
    "- Aizerman, M. A.; Braverman, E. M.; Rozonoer, L. I. (1964). \"Theoretical foundations of the potential function method in pattern recognition learning\". Automation and Remote Control. 25: 821–837.\n",
    " \n",
    "- Mohri, Mehryar and Rostamizadeh, Afshin (2013). Perceptron Mistake Bounds.\n",
    "\n",
    "Source: https://www.kaggle.com/code/vitorgamalemos/perceptron-neural-network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9edccc-3bc7-400f-9b6a-96d2c9ac9580",
   "metadata": {},
   "source": [
    "## Training a Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e6edd5-0ed0-417d-b05d-47d20e46b489",
   "metadata": {},
   "source": [
    "We are going to use a Perceptron from the `sklearn` library to build a simple classification model. We make use of a MultiLayerPerceptron as the name suggest in default it has multiple layers (100). In this excercise we bring this down to a perceptron which is basically a MLP with only one layer, so the variable 'hidden_layer_sizes' will be 1.  \n",
    "Before training, we must prepare the data by applying Standard Scaling (to normalize the data) and One-Hot Encoding (to convert categorical values into a format suitable for the model).  \n",
    "\n",
    "##### 1. Data Preprocessing\n",
    "- One-Hot Encoding converts categorical features into a numerical format by creating binary columns for each category.\n",
    "- Standard scaling transforms features to have a mean of 0 and a standard deviation of 1. This normalization step is crucial for models like Perceptron, which are sensitive to varying scales in input data. Without scaling, features with larger magnitudes can dominate the learning process, leading to poor model performance.\n",
    "\n",
    "##### 2. Splitting the Data\n",
    "Like with Decision Trees or Random Forests, we split our dataset into training and test sets to evaluate model performance.\n",
    "\n",
    "##### 3. Training the Perceptron\n",
    "We initialize the Perceptron model and train it using gradient descent.  \n",
    "The two key parameters we start fine-tuning with are:\n",
    "\n",
    "###### Learning Rate (`eta0`)\n",
    "- Controls how much the model updates its weights during training.\n",
    "- **High learning rate:** Fast convergence, but may overshoot optimal weights.\n",
    "- **Low learning rate:** More stable, but slow training.\n",
    "\n",
    "###### Epochs (`max_iter`)\n",
    "- Defines the number of passes over the dataset during training.\n",
    "- More epochs allow the model to learn better, but too many can cause overfitting.\n",
    "\n",
    "##### 4. Fine-Tuning the Model\n",
    "After training, we can fine-tune hyperparameters like:\n",
    "- Learning rate (`eta0`)\n",
    "- Epochs (`max_iter`)\n",
    "\n",
    "By adjusting these parameters, we aim to improve accuracy and generalization."
   ]
  },
  {
   "cell_type": "code",
   "id": "486a13c1-ff7e-45d6-8514-880a88286ff3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T11:52:45.962944Z",
     "start_time": "2025-05-13T11:52:45.876324Z"
    }
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "b0d6c4b1-138e-4a07-a910-a57e49fcc8dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T11:52:45.976965Z",
     "start_time": "2025-05-13T11:52:45.970012Z"
    }
   },
   "source": [
    "# Load the Iris dataset\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "15b5b8ca-50a9-4e25-addb-164f0ae378b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T11:52:45.988828Z",
     "start_time": "2025-05-13T11:52:45.986619Z"
    }
   },
   "source": [
    "# Preprocess the data (scale the features)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "e5562e30-f5d0-4799-af89-31697f388672",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T11:52:46.002599Z",
     "start_time": "2025-05-13T11:52:46.000263Z"
    }
   },
   "source": [
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "909c6f14-0f33-4149-b1f7-12cde9921dc1",
   "metadata": {},
   "source": [
    "#### Iteration 1"
   ]
  },
  {
   "cell_type": "code",
   "id": "5324b673-ce5e-4547-b1e6-9c63d4471e02",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T11:52:46.027458Z",
     "start_time": "2025-05-13T11:52:46.015756Z"
    }
   },
   "source": [
    "# Set the learning rate, max_iter, and hidden layer sizes as specified\n",
    "learning_rate = 0.001\n",
    "max_iter = 5 # from 5 to 40\n",
    "hidden_layer_sizes = 1\n",
    "\n",
    "# Initialize the MLPClassifier with the given hyperparameters\n",
    "model = MLPClassifier(\n",
    "    hidden_layer_sizes=hidden_layer_sizes,\n",
    "    learning_rate_init=learning_rate,\n",
    "    max_iter=max_iter,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the training set\n",
    "y_train_pred = model.predict(X_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy for both training and test sets\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.3583\n",
      "Test Accuracy: 0.3333\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "a1814f3a-b526-465a-b34d-c6d83b8a253e",
   "metadata": {},
   "source": [
    "#### Iteration 2"
   ]
  },
  {
   "cell_type": "code",
   "id": "4ae8fc00-5031-4908-8510-23828bbd3fb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T11:52:46.049515Z",
     "start_time": "2025-05-13T11:52:46.044504Z"
    }
   },
   "source": [
    "# Set the learning rate, max_iter, and hidden layer sizes as specified\n",
    "learning_rate = 0.05 # from 0.001 to 0.1 like 0.01 or 0.05 \n",
    "max_iter = 4 \n",
    "hidden_layer_sizes = 1\n",
    "\n",
    "# Initialize the MLPClassifier with the given hyperparameters\n",
    "model = MLPClassifier(\n",
    "    hidden_layer_sizes=hidden_layer_sizes,\n",
    "    learning_rate_init=learning_rate,\n",
    "    max_iter=max_iter,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the training set\n",
    "y_train_pred = model.predict(X_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy for both training and test sets\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.4583\n",
      "Test Accuracy: 0.4333\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "d897e171-6a9f-4292-9512-52033ef8c8aa",
   "metadata": {},
   "source": [
    "#### Iteration 3"
   ]
  },
  {
   "cell_type": "code",
   "id": "c3412fbd-0412-4593-9165-62d27a7c4f4d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T11:52:46.073991Z",
     "start_time": "2025-05-13T11:52:46.064482Z"
    }
   },
   "source": [
    "# Set the learning rate, max_iter, and hidden layer sizes as specified\n",
    "learning_rate = 0.1\n",
    "max_iter = 50\n",
    "hidden_layer_sizes = 1\n",
    "\n",
    "# Initialize the MLPClassifier with the given hyperparameters\n",
    "model = MLPClassifier(\n",
    "    hidden_layer_sizes=hidden_layer_sizes,\n",
    "    learning_rate_init=learning_rate,\n",
    "    max_iter=max_iter,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the training set\n",
    "y_train_pred = model.predict(X_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy for both training and test sets\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9750\n",
      "Test Accuracy: 0.9667\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "b4d5178f-d5f6-4113-be4e-9613a3aed107",
   "metadata": {},
   "source": [
    "### Portfolio assignment 20\n",
    "30 min: Train a perceptron to predict the number of the MNIST dataset.\n",
    "- Fit a Perceptron model (keep hidden_layer_sizes=1) using the images in de fetch openml dataset.\n",
    "- Change the learning_rate and max_iter to find the 'right fit'.\n",
    "- Use your perceptron to make predictions for both the train and test set.<br>"
   ]
  },
  {
   "cell_type": "code",
   "id": "4a39c355-8f43-4d8b-8302-f6c1e885f1ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T11:53:01.690043Z",
     "start_time": "2025-05-13T11:52:46.091764Z"
    }
   },
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the MNIST dataset\n",
    "mnist = fetch_openml('mnist_784')\n",
    "X = mnist.data\n",
    "y = mnist.target.astype(int)\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\oldma\\anaconda3\\envs\\datascience\\Lib\\site-packages\\sklearn\\datasets\\_openml.py:110: UserWarning: A network error occurred while downloading https://api.openml.org/api/v1/json/data/list/data_name/mnist_784/limit/2/status/active/. Retrying...\n",
      "  warn(\n"
     ]
    },
    {
     "ename": "URLError",
     "evalue": "<urlopen error [Errno 11001] getaddrinfo failed>",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mgaierror\u001B[39m                                  Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\datascience\\Lib\\urllib\\request.py:1319\u001B[39m, in \u001B[36mAbstractHTTPHandler.do_open\u001B[39m\u001B[34m(self, http_class, req, **http_conn_args)\u001B[39m\n\u001B[32m   1318\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1319\u001B[39m     \u001B[43mh\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreq\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreq\u001B[49m\u001B[43m.\u001B[49m\u001B[43mselector\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreq\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1320\u001B[39m \u001B[43m              \u001B[49m\u001B[43mencode_chunked\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreq\u001B[49m\u001B[43m.\u001B[49m\u001B[43mhas_header\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mTransfer-encoding\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1321\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err: \u001B[38;5;66;03m# timeout error\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\datascience\\Lib\\http\\client.py:1338\u001B[39m, in \u001B[36mHTTPConnection.request\u001B[39m\u001B[34m(self, method, url, body, headers, encode_chunked)\u001B[39m\n\u001B[32m   1337\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Send a complete request to the server.\"\"\"\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1338\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_send_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencode_chunked\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\datascience\\Lib\\http\\client.py:1384\u001B[39m, in \u001B[36mHTTPConnection._send_request\u001B[39m\u001B[34m(self, method, url, body, headers, encode_chunked)\u001B[39m\n\u001B[32m   1383\u001B[39m     body = _encode(body, \u001B[33m'\u001B[39m\u001B[33mbody\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m-> \u001B[39m\u001B[32m1384\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mendheaders\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencode_chunked\u001B[49m\u001B[43m=\u001B[49m\u001B[43mencode_chunked\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\datascience\\Lib\\http\\client.py:1333\u001B[39m, in \u001B[36mHTTPConnection.endheaders\u001B[39m\u001B[34m(self, message_body, encode_chunked)\u001B[39m\n\u001B[32m   1332\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m CannotSendHeader()\n\u001B[32m-> \u001B[39m\u001B[32m1333\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_send_output\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmessage_body\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencode_chunked\u001B[49m\u001B[43m=\u001B[49m\u001B[43mencode_chunked\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\datascience\\Lib\\http\\client.py:1093\u001B[39m, in \u001B[36mHTTPConnection._send_output\u001B[39m\u001B[34m(self, message_body, encode_chunked)\u001B[39m\n\u001B[32m   1092\u001B[39m \u001B[38;5;28;01mdel\u001B[39;00m \u001B[38;5;28mself\u001B[39m._buffer[:]\n\u001B[32m-> \u001B[39m\u001B[32m1093\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmsg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1095\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m message_body \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m   1096\u001B[39m \n\u001B[32m   1097\u001B[39m     \u001B[38;5;66;03m# create a consistent interface to message_body\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\datascience\\Lib\\http\\client.py:1037\u001B[39m, in \u001B[36mHTTPConnection.send\u001B[39m\u001B[34m(self, data)\u001B[39m\n\u001B[32m   1036\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.auto_open:\n\u001B[32m-> \u001B[39m\u001B[32m1037\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mconnect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1038\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\datascience\\Lib\\http\\client.py:1472\u001B[39m, in \u001B[36mHTTPSConnection.connect\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1470\u001B[39m \u001B[33m\"\u001B[39m\u001B[33mConnect to a host on a given (SSL) port.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m-> \u001B[39m\u001B[32m1472\u001B[39m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mconnect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1474\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._tunnel_host:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\datascience\\Lib\\http\\client.py:1003\u001B[39m, in \u001B[36mHTTPConnection.connect\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1002\u001B[39m sys.audit(\u001B[33m\"\u001B[39m\u001B[33mhttp.client.connect\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28mself\u001B[39m, \u001B[38;5;28mself\u001B[39m.host, \u001B[38;5;28mself\u001B[39m.port)\n\u001B[32m-> \u001B[39m\u001B[32m1003\u001B[39m \u001B[38;5;28mself\u001B[39m.sock = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_create_connection\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1004\u001B[39m \u001B[43m    \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mhost\u001B[49m\u001B[43m,\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mport\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43msource_address\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1005\u001B[39m \u001B[38;5;66;03m# Might fail in OSs that don't implement TCP_NODELAY\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\datascience\\Lib\\socket.py:840\u001B[39m, in \u001B[36mcreate_connection\u001B[39m\u001B[34m(address, timeout, source_address, all_errors)\u001B[39m\n\u001B[32m    839\u001B[39m exceptions = []\n\u001B[32m--> \u001B[39m\u001B[32m840\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m res \u001B[38;5;129;01min\u001B[39;00m \u001B[43mgetaddrinfo\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhost\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mport\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mSOCK_STREAM\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[32m    841\u001B[39m     af, socktype, proto, canonname, sa = res\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\datascience\\Lib\\socket.py:977\u001B[39m, in \u001B[36mgetaddrinfo\u001B[39m\u001B[34m(host, port, family, type, proto, flags)\u001B[39m\n\u001B[32m    976\u001B[39m addrlist = []\n\u001B[32m--> \u001B[39m\u001B[32m977\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m res \u001B[38;5;129;01min\u001B[39;00m \u001B[43m_socket\u001B[49m\u001B[43m.\u001B[49m\u001B[43mgetaddrinfo\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhost\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mport\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfamily\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mtype\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mproto\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mflags\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[32m    978\u001B[39m     af, socktype, proto, canonname, sa = res\n",
      "\u001B[31mgaierror\u001B[39m: [Errno 11001] getaddrinfo failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[31mURLError\u001B[39m                                  Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[10]\u001B[39m\u001B[32m, line 5\u001B[39m\n\u001B[32m      2\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mseaborn\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msns\u001B[39;00m\n\u001B[32m      4\u001B[39m \u001B[38;5;66;03m# Load the MNIST dataset\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m5\u001B[39m mnist = \u001B[43mfetch_openml\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mmnist_784\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m      6\u001B[39m X = mnist.data\n\u001B[32m      7\u001B[39m y = mnist.target.astype(\u001B[38;5;28mint\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\datascience\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001B[39m, in \u001B[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    210\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    211\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[32m    212\u001B[39m         skip_parameter_validation=(\n\u001B[32m    213\u001B[39m             prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[32m    214\u001B[39m         )\n\u001B[32m    215\u001B[39m     ):\n\u001B[32m--> \u001B[39m\u001B[32m216\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    217\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m InvalidParameterError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    218\u001B[39m     \u001B[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001B[39;00m\n\u001B[32m    219\u001B[39m     \u001B[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001B[39;00m\n\u001B[32m    220\u001B[39m     \u001B[38;5;66;03m# the name of the estimator by the name of the function in the error\u001B[39;00m\n\u001B[32m    221\u001B[39m     \u001B[38;5;66;03m# message to avoid confusion.\u001B[39;00m\n\u001B[32m    222\u001B[39m     msg = re.sub(\n\u001B[32m    223\u001B[39m         \u001B[33mr\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mparameter of \u001B[39m\u001B[33m\\\u001B[39m\u001B[33mw+ must be\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    224\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mparameter of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc.\u001B[34m__qualname__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m must be\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    225\u001B[39m         \u001B[38;5;28mstr\u001B[39m(e),\n\u001B[32m    226\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\datascience\\Lib\\site-packages\\sklearn\\datasets\\_openml.py:1011\u001B[39m, in \u001B[36mfetch_openml\u001B[39m\u001B[34m(name, version, data_id, data_home, target_column, cache, return_X_y, as_frame, n_retries, delay, parser, read_csv_kwargs)\u001B[39m\n\u001B[32m   1005\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m data_id \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m   1006\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m   1007\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mDataset data_id=\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[33m and name=\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[33m passed, but you can only \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1008\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mspecify a numeric data_id or a name, not \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1009\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mboth.\u001B[39m\u001B[33m\"\u001B[39m.format(data_id, name)\n\u001B[32m   1010\u001B[39m         )\n\u001B[32m-> \u001B[39m\u001B[32m1011\u001B[39m     data_info = \u001B[43m_get_data_info_by_name\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1012\u001B[39m \u001B[43m        \u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mversion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata_home\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_retries\u001B[49m\u001B[43m=\u001B[49m\u001B[43mn_retries\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdelay\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdelay\u001B[49m\n\u001B[32m   1013\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1014\u001B[39m     data_id = data_info[\u001B[33m\"\u001B[39m\u001B[33mdid\u001B[39m\u001B[33m\"\u001B[39m]\n\u001B[32m   1015\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m data_id \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m   1016\u001B[39m     \u001B[38;5;66;03m# from the previous if statement, it is given that name is None\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\datascience\\Lib\\site-packages\\sklearn\\datasets\\_openml.py:302\u001B[39m, in \u001B[36m_get_data_info_by_name\u001B[39m\u001B[34m(name, version, data_home, n_retries, delay)\u001B[39m\n\u001B[32m    300\u001B[39m url = _SEARCH_NAME.format(name) + \u001B[33m\"\u001B[39m\u001B[33m/status/active/\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    301\u001B[39m error_msg = \u001B[33m\"\u001B[39m\u001B[33mNo active dataset \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[33m found.\u001B[39m\u001B[33m\"\u001B[39m.format(name)\n\u001B[32m--> \u001B[39m\u001B[32m302\u001B[39m json_data = \u001B[43m_get_json_content_from_openml_api\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    303\u001B[39m \u001B[43m    \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    304\u001B[39m \u001B[43m    \u001B[49m\u001B[43merror_msg\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    305\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdata_home\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdata_home\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    306\u001B[39m \u001B[43m    \u001B[49m\u001B[43mn_retries\u001B[49m\u001B[43m=\u001B[49m\u001B[43mn_retries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    307\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdelay\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdelay\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    308\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    309\u001B[39m res = json_data[\u001B[33m\"\u001B[39m\u001B[33mdata\u001B[39m\u001B[33m\"\u001B[39m][\u001B[33m\"\u001B[39m\u001B[33mdataset\u001B[39m\u001B[33m\"\u001B[39m]\n\u001B[32m    310\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(res) > \u001B[32m1\u001B[39m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\datascience\\Lib\\site-packages\\sklearn\\datasets\\_openml.py:246\u001B[39m, in \u001B[36m_get_json_content_from_openml_api\u001B[39m\u001B[34m(url, error_message, data_home, n_retries, delay)\u001B[39m\n\u001B[32m    243\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m json.loads(response.read().decode(\u001B[33m\"\u001B[39m\u001B[33mutf-8\u001B[39m\u001B[33m\"\u001B[39m))\n\u001B[32m    245\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m246\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_load_json\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    247\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m HTTPError \u001B[38;5;28;01mas\u001B[39;00m error:\n\u001B[32m    248\u001B[39m     \u001B[38;5;66;03m# 412 is an OpenML specific error code, indicating a generic error\u001B[39;00m\n\u001B[32m    249\u001B[39m     \u001B[38;5;66;03m# (e.g., data not found)\u001B[39;00m\n\u001B[32m    250\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m error.code != \u001B[32m412\u001B[39m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\datascience\\Lib\\site-packages\\sklearn\\datasets\\_openml.py:67\u001B[39m, in \u001B[36m_retry_with_clean_cache.<locals>.decorator.<locals>.wrapper\u001B[39m\u001B[34m(*args, **kw)\u001B[39m\n\u001B[32m     65\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m f(*args, **kw)\n\u001B[32m     66\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m67\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     68\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m URLError:\n\u001B[32m     69\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\datascience\\Lib\\site-packages\\sklearn\\datasets\\_openml.py:241\u001B[39m, in \u001B[36m_get_json_content_from_openml_api.<locals>._load_json\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m    238\u001B[39m \u001B[38;5;129m@_retry_with_clean_cache\u001B[39m(url, data_home=data_home)\n\u001B[32m    239\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_load_json\u001B[39m():\n\u001B[32m    240\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m closing(\n\u001B[32m--> \u001B[39m\u001B[32m241\u001B[39m         \u001B[43m_open_openml_url\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata_home\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_retries\u001B[49m\u001B[43m=\u001B[49m\u001B[43mn_retries\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdelay\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdelay\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    242\u001B[39m     ) \u001B[38;5;28;01mas\u001B[39;00m response:\n\u001B[32m    243\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m json.loads(response.read().decode(\u001B[33m\"\u001B[39m\u001B[33mutf-8\u001B[39m\u001B[33m\"\u001B[39m))\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\datascience\\Lib\\site-packages\\sklearn\\datasets\\_openml.py:173\u001B[39m, in \u001B[36m_open_openml_url\u001B[39m\u001B[34m(openml_path, data_home, n_retries, delay)\u001B[39m\n\u001B[32m    166\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    167\u001B[39m     \u001B[38;5;66;03m# Create a tmpdir as a subfolder of dir_name where the final file will\u001B[39;00m\n\u001B[32m    168\u001B[39m     \u001B[38;5;66;03m# be moved to if the download is successful. This guarantees that the\u001B[39;00m\n\u001B[32m    169\u001B[39m     \u001B[38;5;66;03m# renaming operation to the final location is atomic to ensure the\u001B[39;00m\n\u001B[32m    170\u001B[39m     \u001B[38;5;66;03m# concurrence safety of the dataset caching mechanism.\u001B[39;00m\n\u001B[32m    171\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m TemporaryDirectory(\u001B[38;5;28mdir\u001B[39m=dir_name) \u001B[38;5;28;01mas\u001B[39;00m tmpdir:\n\u001B[32m    172\u001B[39m         \u001B[38;5;28;01mwith\u001B[39;00m closing(\n\u001B[32m--> \u001B[39m\u001B[32m173\u001B[39m             \u001B[43m_retry_on_network_error\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn_retries\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdelay\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreq\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfull_url\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[43murlopen\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    174\u001B[39m \u001B[43m                \u001B[49m\u001B[43mreq\u001B[49m\n\u001B[32m    175\u001B[39m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    176\u001B[39m         ) \u001B[38;5;28;01mas\u001B[39;00m fsrc:\n\u001B[32m    177\u001B[39m             opener: Callable\n\u001B[32m    178\u001B[39m             \u001B[38;5;28;01mif\u001B[39;00m is_gzip_encoded(fsrc):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\datascience\\Lib\\site-packages\\sklearn\\datasets\\_openml.py:103\u001B[39m, in \u001B[36m_retry_on_network_error.<locals>.decorator.<locals>.wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    101\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[32m    102\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m103\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    104\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m (URLError, \u001B[38;5;167;01mTimeoutError\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    105\u001B[39m         \u001B[38;5;66;03m# 412 is a specific OpenML error code.\u001B[39;00m\n\u001B[32m    106\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(e, HTTPError) \u001B[38;5;129;01mand\u001B[39;00m e.code == \u001B[32m412\u001B[39m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\datascience\\Lib\\urllib\\request.py:189\u001B[39m, in \u001B[36murlopen\u001B[39m\u001B[34m(url, data, timeout, context)\u001B[39m\n\u001B[32m    187\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    188\u001B[39m     opener = _opener\n\u001B[32m--> \u001B[39m\u001B[32m189\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mopener\u001B[49m\u001B[43m.\u001B[49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\datascience\\Lib\\urllib\\request.py:489\u001B[39m, in \u001B[36mOpenerDirector.open\u001B[39m\u001B[34m(self, fullurl, data, timeout)\u001B[39m\n\u001B[32m    486\u001B[39m     req = meth(req)\n\u001B[32m    488\u001B[39m sys.audit(\u001B[33m'\u001B[39m\u001B[33murllib.Request\u001B[39m\u001B[33m'\u001B[39m, req.full_url, req.data, req.headers, req.get_method())\n\u001B[32m--> \u001B[39m\u001B[32m489\u001B[39m response = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    491\u001B[39m \u001B[38;5;66;03m# post-process response\u001B[39;00m\n\u001B[32m    492\u001B[39m meth_name = protocol+\u001B[33m\"\u001B[39m\u001B[33m_response\u001B[39m\u001B[33m\"\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\datascience\\Lib\\urllib\\request.py:506\u001B[39m, in \u001B[36mOpenerDirector._open\u001B[39m\u001B[34m(self, req, data)\u001B[39m\n\u001B[32m    503\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n\u001B[32m    505\u001B[39m protocol = req.type\n\u001B[32m--> \u001B[39m\u001B[32m506\u001B[39m result = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_chain\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mhandle_open\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprotocol\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprotocol\u001B[49m\u001B[43m \u001B[49m\u001B[43m+\u001B[49m\n\u001B[32m    507\u001B[39m \u001B[43m                          \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m_open\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreq\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    508\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m result:\n\u001B[32m    509\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\datascience\\Lib\\urllib\\request.py:466\u001B[39m, in \u001B[36mOpenerDirector._call_chain\u001B[39m\u001B[34m(self, chain, kind, meth_name, *args)\u001B[39m\n\u001B[32m    464\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m handler \u001B[38;5;129;01min\u001B[39;00m handlers:\n\u001B[32m    465\u001B[39m     func = \u001B[38;5;28mgetattr\u001B[39m(handler, meth_name)\n\u001B[32m--> \u001B[39m\u001B[32m466\u001B[39m     result = \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    467\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m result \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    468\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\datascience\\Lib\\urllib\\request.py:1367\u001B[39m, in \u001B[36mHTTPSHandler.https_open\u001B[39m\u001B[34m(self, req)\u001B[39m\n\u001B[32m   1366\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mhttps_open\u001B[39m(\u001B[38;5;28mself\u001B[39m, req):\n\u001B[32m-> \u001B[39m\u001B[32m1367\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdo_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhttp\u001B[49m\u001B[43m.\u001B[49m\u001B[43mclient\u001B[49m\u001B[43m.\u001B[49m\u001B[43mHTTPSConnection\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreq\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1368\u001B[39m \u001B[43m                        \u001B[49m\u001B[43mcontext\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_context\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\datascience\\Lib\\urllib\\request.py:1322\u001B[39m, in \u001B[36mAbstractHTTPHandler.do_open\u001B[39m\u001B[34m(self, http_class, req, **http_conn_args)\u001B[39m\n\u001B[32m   1319\u001B[39m         h.request(req.get_method(), req.selector, req.data, headers,\n\u001B[32m   1320\u001B[39m                   encode_chunked=req.has_header(\u001B[33m'\u001B[39m\u001B[33mTransfer-encoding\u001B[39m\u001B[33m'\u001B[39m))\n\u001B[32m   1321\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err: \u001B[38;5;66;03m# timeout error\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1322\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m URLError(err)\n\u001B[32m   1323\u001B[39m     r = h.getresponse()\n\u001B[32m   1324\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m:\n",
      "\u001B[31mURLError\u001B[39m: <urlopen error [Errno 11001] getaddrinfo failed>"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "fe5b61c2-aa2f-4b9b-b547-1cd4c4c3d04c",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/0v1CGNV.png)<br>\n",
    "- Fine-tune the learning rate and epochs (max iterations).\n",
    "- Calculate the accuracy for both the train set predictions and test set predictions, what happens per iteration?\n",
    "- Is the accurracy different? Did you expect this difference?\n",
    "\n",
    "Optional: Perform the same tasks but change the hidden_layer_sizes <br>\n",
    "\n",
    "Findings: ...<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5aff5e-b2fa-4e43-8470-9997e3217120",
   "metadata": {},
   "source": [
    "### Portfolio assignment 21\n",
    "30 min: Train a perceptron to predict one of the categorical columns of your own dataset.\n",
    "- Prepare the data:<br>\n",
    "    - <b>Note</b>: Some machine learning algorithms can not handle missing values. You will either need to: \n",
    "         - replace missing values (with the mean or most popular value). For replacing missing values you can use .fillna(\\<value\\>) https://pandas.pydata.org/docs/reference/api/pandas.Series.fillna.html\n",
    "         - remove rows with missing data.  You can remove rows with missing data with .dropna() https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html <br>\n",
    "    - <b>Note</b>: Some machine learning algorithms can not handle categorical values. You will either need to:\n",
    "        -  To handle categorical data, you can use One-Hot Encoding to convert categories into binary columns with .get_dummies(<value(s)>). This creates a new column for each category\n",
    "- Split your dataset into training (70%) and testing (30%) sets. \n",
    "- Use your Perceptron to make predictions for both the train and test set.<br>\n",
    "<br>\n",
    "\n",
    "![](https://i.imgur.com/0v1CGNV.png)<br>\n",
    "- Fit a Perceptron model using your own selected feature columns.\n",
    "- Fine-tune the learning rate and epochs (max iterations). \n",
    "- Calculate the accuracy for both the train set predictions and test set predictions, what happens per iteration?\n",
    "- Is the accurracy different? Did you expect this difference?\n",
    "\n",
    "Optional: Perform the same tasks but change the hidden_layer_sizes <br>\n",
    "\n",
    "Findings: ...<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
